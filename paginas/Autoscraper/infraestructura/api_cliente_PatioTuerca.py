from __future__ import annotations
from typing import Protocol, Dict, Any, List
from bs4 import BeautifulSoup
from paginas.Autoscraper.dominio.modelo import Vehiculo
import requests
import re
import time, base64, json

URL_BASE = "https://ecuador.patiotuerca.com/usados/-/autos"

#-------------------------Web-----------------------------------
class WebClient(Protocol):
    def fetch_html(self, url: str) -> str: ...

class RequestsWebClient(WebClient):
    def __init__(self, user_agent: str, timeout: int = 15):
        self.timeout = timeout
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": user_agent})

    def fetch_html(self, url: str) -> str:
        resp = self.session.get(url, timeout=self.timeout)
        resp.raise_for_status()
        return resp.text

#---------------------------Extracci√≥n de las urls para sacar data---------------------------
def generar_codigo_base64(n: int) -> str:
    """Codifica un n√∫mero de p√°gina en base64, como usa PatioTuerca."""
    return base64.b64encode(str(n).encode()).decode()


class PatioTuercaRepositorio():
    """Repositorio que obtiene veh√≠culos por a√±o desde PatioTuerca."""
    def __init__(self, web_client: RequestsWebClient, pausa: int = 5, num_paginas: int = 10): #modificar la cantidad de p√°ginas o el tiempo de pausa aqu√≠ de ser necesario.
        self.web = web_client
        self.num_paginas = num_paginas
        self.pausa = pausa

    def _extraer_urls_vehiculos(self, url_pagina: str) -> List[str]:
        """Extrae URLs de fichas de veh√≠culos a partir del JSON-LD embebido en la p√°gina."""
        html = self.web.fetch_html(url_pagina)
        soup = BeautifulSoup(html, "html.parser")

        urls = []
        for script in soup.find_all("script", {"type": "application/ld+json"}):
            try:
                data = json.loads(script.string)
                # Algunos scripts tienen una lista de objetos, otros un dict
                if isinstance(data, list):
                    for item in data:
                        if item.get("@type") == "Car" and "url" in item:
                            urls.append(item["url"])
                elif isinstance(data, dict):
                    if data.get("@type") == "Car" and "url" in data:
                        urls.append(data["url"])
            except Exception:
                continue

        return urls

    def obtener_vehiculos_por_anio(self, anio: int) -> List[Vehiculo]:
        """Recorre las p√°ginas de resultados para un a√±o espec√≠fico y extrae las fichas completas."""
        print(f"üöó Buscando veh√≠culos del a√±o {anio}...")
        base_url = f"{URL_BASE}/-/-/-/{anio}"
        todas_urls = []

        for pagina in range(1, self.num_paginas + 1):
            # Construye la URL de la p√°gina actual
            if pagina == 1:
                url_pagina = base_url
            else:
                codigo = generar_codigo_base64(pagina - 1)
                url_pagina = f"{base_url}?page={codigo}"

            print(f"üîé P√°gina {pagina}: {url_pagina}")
            try:
                urls = self._extraer_urls_vehiculos(url_pagina)
                if not urls:
                    print(f"‚ö†Ô∏è No hay m√°s resultados para {anio}")
                    break
                todas_urls.extend(urls)
                print(f"‚úÖ {len(urls)} URLs encontradas en p√°gina {pagina}")
                time.sleep(self.pausa)
            except Exception as e:
                print(f"‚ùå Error en p√°gina {pagina}: {e}")
                break

        # Extrae las fichas completas de cada URL
        vehiculos = []
        for i, url in enumerate(todas_urls, start=1):
            try:
                html = self.web.fetch_html(url)
                ficha = FichaExtractor.parsear_html(html, url)
                if not ficha["id"]:
                    continue
                vehiculos.append(Vehiculo(
                id=ficha["id"],
                summary=ficha["summary"],
                ficha_tecnica=ficha["ficha_tecnica"],
                url = ficha["url"]
                ))
                print(f"   üîπ {i}/{len(todas_urls)}: {ficha['id']} OK")
                time.sleep(0.8)
            except Exception as e:
                print(f"   ‚ö†Ô∏è Error al procesar {url}: {e}")

        print(f"üìä Total extra√≠dos para {anio}: {len(vehiculos)} veh√≠culos")
        return vehiculos


#--------------------Extracci√≥n de la Data------------------------
class FichaExtractor:
    @staticmethod
    def extraer_id(soup: BeautifulSoup, url: str) -> str | None:
        meta_id = soup.find("meta", {"itemprop": "productID"})
        if meta_id and meta_id.get("content"):
            return meta_id["content"]

        match = re.search(r"/(\d+)$", url)
        return match.group(1) if match else None

    @staticmethod
    def extraer_summary(soup: BeautifulSoup) -> Dict[str, Any]:
        data = {}
        section = soup.find("section", id="summary")
        if not section:
            return data

        for div in section.find_all("div", class_="col"):
            small = div.find("small")
            if not small:
                continue
            nombre = small.get_text(strip=True)
            valor = div.get_text(strip=True).replace(nombre, "").strip()
            data[nombre] = valor
        return data

    @staticmethod
    def extraer_ficha_tecnica(soup: BeautifulSoup) -> Dict[str, Any]:
        data = {}
        ficha = soup.find("section", id="technicalData")
        if not ficha:
            return data
        
        for p in ficha.find_all("p", class_="m-none"):
            nombre = p.find("small")
            valor = p.find("span")
            if nombre and valor:
                data[nombre.get_text(strip=True)] = valor.get_text(strip=True)
        return data

    @staticmethod
    def parsear_html(html: str, url: str) -> Dict[str, Any]:
        soup = BeautifulSoup(html, "html.parser")

        return {
            "id": FichaExtractor.extraer_id(soup, url),
            "summary": FichaExtractor.extraer_summary(soup),
            "ficha_tecnica": FichaExtractor.extraer_ficha_tecnica(soup),
            "url": url
        }
    
#---------------------------------Adaptador-----------------------------
class PatioTuercaClientAdapter:
    """Adaptador que expone la misma interfaz de un ApiClient est√°ndar."""
    def __init__(self, web_client: RequestsWebClient, anios: list[int]):
        self.repo = PatioTuercaRepositorio(web_client)
        self.anios = anios

    def fetch_year(self, anio: int) -> List[Dict[str, Any]]:
        """Devuelve las fichas de veh√≠culos de un solo a√±o."""
        vehiculos = self.repo.obtener_vehiculos_por_anio(anio)

        entities: List[Dict[str, Any]] = []
        for v in vehiculos:
            entities.append({
                "id_record": v.id,
                "summary": v.summary,
                "ficha_tecnica": v.ficha_tecnica,
                "url": v.url
            })
        return entities

    def fetch_all(self) -> List[Dict[str, Any]]:
        """Devuelve la lista de fichas de veh√≠culos de todos los a√±os indicados."""
        all_entities: List[Dict[str, Any]] = []
        for anio in self.anios:
            vehiculos = self.repo.obtener_vehiculos_por_anio(anio)
            # Convertimos los Vehiculo (dataclasses o dicts) a diccionarios simples
            for v in vehiculos:
                all_entities.append({
                    "id": v.id,
                    "summary": v.summary,
                    "ficha_tecnica": v.ficha_tecnica,
                    "url":v.url
                })
        return all_entities
